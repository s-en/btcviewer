{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from db import DB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = DB(20201114, 4)\n",
    "EXC = ['bitflyer', 'liquid', 'zaif', 'coincheck']\n",
    "baseTime = datetime(2020, 11, 14, 0, 0, 0).timestamp()\n",
    "\n",
    "def ml(func, array):\n",
    "    return list(map(func, array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "NONE = 0\n",
    "BUY = 1\n",
    "SELL = 2\n",
    "\n",
    "BUYAMOUNT = 0.01\n",
    "DIVP = 1000000\n",
    "EXLEN = len(EXC)\n",
    "N_ACTIONS = 8\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    self.action_space = spaces.MultiBinary(N_ACTIONS)\n",
    "    self.observation_space = spaces.Box(low=0, high=100,\n",
    "                                        shape=(968,),dtype=np.float32)\n",
    "    self.asks = {}\n",
    "    self.bids = {}\n",
    "    self.maxCnt = 0\n",
    "    for ex in EXC:\n",
    "      prices = mydb.get(ex, baseTime, baseTime + 60*60*24)\n",
    "      # example for self.asks[ex]\n",
    "      # [[[1701256.0, 0.0001], [1701501.0, 0.005], [1701505.0, 0.01015336]],\n",
    "      # [[1701316.0, 0.01], [1701494.0, 0.01275137], [1701500.0, 0.02]],\n",
    "      # [[1702132.0, 0.02440992], [1702180.0, 0.1], [1702290.0, 0.08]]]\n",
    "      self.asks[ex] = ml(lambda p:p['asks'], prices)\n",
    "      self.bids[ex] = ml(lambda p:p['bids'], prices)\n",
    "      self.maxCnt = max([self.maxCnt, len(self.asks[ex])])\n",
    "    self.initVal()\n",
    "\n",
    "  def initVal(self):\n",
    "    self.episode_ended = False\n",
    "    # ランダム時間から開始\n",
    "    self.stepCnt = np.random.randint(0, self.maxCnt-129)\n",
    "    self.startCnt = self.stepCnt\n",
    "    # ランダムな所持金で開始\n",
    "    jpy = (np.random.randint(10000, 1000000, EXLEN) / DIVP).tolist()\n",
    "    btc = (abs(np.random.randn(EXLEN))+0.01).tolist()\n",
    "    self.state = jpy + btc + [0]*960\n",
    "    self.step(np.array([0]*N_ACTIONS))\n",
    "    self.initJPY = self.valuationJPY(self.state)\n",
    "    self.initBTC = self.valuationBTC(self.state)\n",
    "\n",
    "  def reset(self):\n",
    "    self.initVal()\n",
    "    return np.array(self.state).astype(np.float32)\n",
    "\n",
    "  # 円換算評価額\n",
    "  def valuationJPY(self, state):\n",
    "    total = 0\n",
    "    for i in range(EXLEN):\n",
    "      jpy = self.state[i]\n",
    "      btc = self.state[EXLEN + i]\n",
    "      ask = self.state[int(EXLEN*2)]\n",
    "      #total += jpy + btc * ask\n",
    "      total += jpy\n",
    "    return total\n",
    "\n",
    "  # BTC換算評価額\n",
    "  def valuationBTC(self, state):\n",
    "    total = 0\n",
    "    for i in range(EXLEN):\n",
    "      jpy = self.state[i]\n",
    "      btc = self.state[EXLEN + i]\n",
    "      ask = self.state[int(EXLEN*2)]\n",
    "      #total += btc\n",
    "      if ask > 0:\n",
    "        total += jpy / ask\n",
    "    return total\n",
    "\n",
    "  def trade(self, exidx, buy):\n",
    "    cnt = self.stepCnt\n",
    "    ex = EXC[exidx]\n",
    "    jpyBal = self.state[0:EXLEN]\n",
    "    btcBal = self.state[EXLEN:EXLEN*2]\n",
    "    # 売買 次のフレームの価格で購入できる\n",
    "    if buy:\n",
    "      print('buy')\n",
    "      print(jpyBal[exidx])\n",
    "      target = self.asks[ex][cnt+1:cnt+2][0]\n",
    "      # 買える分だけ買う\n",
    "      remain = BUYAMOUNT\n",
    "      for dep in range(3):\n",
    "        price = target[dep][0]\n",
    "        amount = target[dep][1]\n",
    "        if remain <= amount:\n",
    "          amount = remain\n",
    "        jpy = price * amount\n",
    "        if jpyBal[exidx] < jpy:\n",
    "          break\n",
    "        jpyBal[exidx] -= jpy\n",
    "        btcBal[exidx] += amount\n",
    "        remain -= amount\n",
    "        if remain <= 0:\n",
    "          break\n",
    "      print(jpyBal[exidx])\n",
    "    else:\n",
    "      target = self.bids[ex][cnt+1:cnt+2][0]\n",
    "      # 売れる分だけ売る\n",
    "      remain = BUYAMOUNT\n",
    "      for dep in range(3):\n",
    "        price = target[dep][0]\n",
    "        amount = target[dep][1]\n",
    "        if remain <= amount:\n",
    "          amount = remain\n",
    "        if btcBal[exidx] < amount:\n",
    "          break\n",
    "        jpyBal[exidx] += price * amount\n",
    "        btcBal[exidx] -= amount\n",
    "        remain -= amount\n",
    "        if remain <= 0:\n",
    "          break\n",
    "    self.state[0:EXLEN] = jpyBal\n",
    "    self.state[EXLEN:EXLEN*2] = btcBal\n",
    "\n",
    "  def step(self, action):\n",
    "    cnt = self.stepCnt\n",
    "    # stateから情報を抜出\n",
    "    board = self.state[EXLEN*2:EXLEN*2+960]\n",
    "\n",
    "    # 1フレーム前の板情報を反映\n",
    "    frameSize = int(len(board) / 20)\n",
    "    board[frameSize:len(board)] = board[0:(len(board)-frameSize)]\n",
    "\n",
    "    # 最新板情報を更新\n",
    "    cur = 0\n",
    "    for ex in EXC:\n",
    "      for ab in [self.asks, self.bids]:\n",
    "        for dep in range(3):\n",
    "          # 金額\n",
    "          board[cur] = ab[ex][cnt:cnt+1][0][dep][0]\n",
    "          cur += 1\n",
    "          # 量\n",
    "          board[cur] = ab[ex][cnt:cnt+1][0][dep][1]\n",
    "          cur += 1\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    for i in range(len(action)):\n",
    "      if action[i] == 1:\n",
    "        exidx = i % EXLEN\n",
    "        buy = i < EXLEN\n",
    "        self.trade(exidx, buy)\n",
    "\n",
    "    # state更新\n",
    "    jpyBal = self.state[0:EXLEN]\n",
    "    btcBal = self.state[EXLEN:EXLEN*2]\n",
    "    self.state = jpyBal + btcBal + board\n",
    "\n",
    "    # 3秒時間を進める\n",
    "    self.stepCnt += 1\n",
    "    if self.stepCnt >= self.maxCnt:\n",
    "      self.episode_ended = True\n",
    "\n",
    "    # 最大128フレームで終了\n",
    "    frameCnt = self.stepCnt - self.startCnt\n",
    "    if frameCnt > 128:\n",
    "      self.episode_ended = True\n",
    "    \n",
    "    reward = 0\n",
    "    if frameCnt > 1:\n",
    "      jpy = self.valuationJPY(self.state)\n",
    "      btc = self.valuationBTC(self.state)\n",
    "      # 初期所持金から何倍増えたかが報酬\n",
    "      reward = jpy / self.initJPY + btc / self.initBTC - 2\n",
    "\n",
    "    # 5%所持金が減ったら強制終了\n",
    "    #if reward < -0.1:\n",
    "    #  self._episode_ended = True\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array(self.state).astype(np.float32), reward*128, self.episode_ended, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\"render\")\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "buy\n0.877338\n0.8606522\nbuy\n0.590124\n0.573424882\nbuy\n0.963661\n0.94696256\nbuy\n0.263175\n0.24648326999999998\nbuy\n0.94696256\n0.93026728\nbuy\n0.87733495\n0.86064915\nbuy\n0.8773319199999999\n0.86064612\nbuy\n0.6234976819999999\n0.6067987819999999\nbuy\n0.8773288899999999\n0.8606430899999999\nbuy\n0.29655340999999996\n0.27986115999999994\nbuy\n0.980338\n0.963642\nbuy\n0.29655015999999995\n0.27985803\nbuy\n0.963642\n0.94694623\nbuy\n0.6401809319999999\n0.623481832\nbuy\n0.96363679\n0.94694111\nbuy\n0.9636312\n0.94693128168816\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100):\n",
    "    \"\"\"\n",
    "    RLエージェントを評価\n",
    "    :param model: (BaseRLModel object) RLエージェント\n",
    "    :param num_episodes: (int) エピソード数\n",
    "    :return: (float) 平均報酬\n",
    "    \"\"\"\n",
    "    # この関数は単一の環境でのみ機能します\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _statesは、LSTMポリシーを使用する場合にのみ有用です\n",
    "            action, _states = model.predict(obs)\n",
    "\n",
    "            # ベクトル化環境を使用しているため、行動、報酬、エピソード完了は配列です\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011087762 |\n",
      "|    clip_fraction        | 0.0596      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | -0.769      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 584          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046734763 |\n",
      "|    clip_fraction        | 0.0673       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.54        |\n",
      "|    explained_variance   | -5.5e+03     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 168          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 383          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012221866 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | -0.265      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070188823 |\n",
      "|    clip_fraction        | 0.0949       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.53        |\n",
      "|    explained_variance   | -0.842       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00602     |\n",
      "|    value_loss           | 3.51         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 491         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009550644 |\n",
      "|    clip_fraction        | 0.0913      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | -1.24       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "Mean reward: 2.737902 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "mean_reward = evaluate(model, num_episodes=100)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    print(action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}