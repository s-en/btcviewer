{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from db import DB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = DB(20201114, 4)\n",
    "EXC = ['bitflyer', 'liquid', 'zaif', 'coincheck']\n",
    "baseTime = datetime(2020, 11, 14, 0, 0, 0).timestamp()\n",
    "\n",
    "def ml(func, array):\n",
    "    return list(map(func, array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "NONE = 0\n",
    "BUY = 1\n",
    "SELL = 2\n",
    "\n",
    "BUYAMOUNT = 0.01\n",
    "DIVP = 1000000\n",
    "EXLEN = len(EXC)\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    n_actions = 9\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    self.observation_space = spaces.Box(low=0, high=100,\n",
    "                                        shape=(968,),dtype=np.float32)\n",
    "    self.asks = {}\n",
    "    self.bids = {}\n",
    "    self.maxCnt = 0\n",
    "    for ex in EXC:\n",
    "      prices = mydb.get(ex, baseTime, baseTime + 60*60*24)\n",
    "      # example for self.asks[ex]\n",
    "      # [[[1701256.0, 0.0001], [1701501.0, 0.005], [1701505.0, 0.01015336]],\n",
    "      # [[1701316.0, 0.01], [1701494.0, 0.01275137], [1701500.0, 0.02]],\n",
    "      # [[1702132.0, 0.02440992], [1702180.0, 0.1], [1702290.0, 0.08]]]\n",
    "      self.asks[ex] = ml(lambda p:p['asks'], prices)\n",
    "      self.bids[ex] = ml(lambda p:p['bids'], prices)\n",
    "      self.maxCnt = max([self.maxCnt, len(self.asks[ex])])\n",
    "    self.initVal()\n",
    "\n",
    "  def initVal(self):\n",
    "    self.episode_ended = False\n",
    "    # ランダム時間から開始\n",
    "    self.stepCnt = np.random.randint(0, self.maxCnt-129)\n",
    "    self.startCnt = self.stepCnt\n",
    "    # ランダムな所持金で開始\n",
    "    jpy = (np.random.randint(10000, 1000000, EXLEN) / DIVP).tolist()\n",
    "    btc = (abs(np.random.randn(EXLEN))+0.01).tolist()\n",
    "    self.state = jpy + btc + [0]*960\n",
    "    self.step(0)\n",
    "    self.initJPY = self.valuationJPY(self.state)\n",
    "    self.initBTC = self.valuationBTC(self.state)\n",
    "\n",
    "  def reset(self):\n",
    "    self.initVal()\n",
    "    return np.array(self.state).astype(np.float32)\n",
    "\n",
    "  # 円換算評価額\n",
    "  def valuationJPY(self, state):\n",
    "    total = 0\n",
    "    for i in range(EXLEN):\n",
    "      jpy = self.state[i]\n",
    "      btc = self.state[EXLEN + i]\n",
    "      ask = self.state[int(EXLEN*2)]\n",
    "      #total += jpy + btc * ask\n",
    "      total += jpy\n",
    "    return total\n",
    "\n",
    "  # BTC換算評価額\n",
    "  def valuationBTC(self, state):\n",
    "    total = 0\n",
    "    for i in range(EXLEN):\n",
    "      jpy = self.state[i]\n",
    "      btc = self.state[EXLEN + i]\n",
    "      ask = self.state[int(EXLEN*2)]\n",
    "      #total += btc\n",
    "      if ask > 0:\n",
    "        total += jpy / ask\n",
    "    return total\n",
    "\n",
    "  def step(self, action):\n",
    "    cnt = self.stepCnt\n",
    "    # stateから情報を抜出\n",
    "    jpyBal = self.state[0:EXLEN]\n",
    "    btcBal = self.state[EXLEN:EXLEN*2]\n",
    "    board = self.state[EXLEN*2:EXLEN*2+960]\n",
    "\n",
    "    # 1フレーム前の板情報を反映\n",
    "    frameSize = int(len(board) / 20)\n",
    "    board[frameSize:len(board)] = board[0:(len(board)-frameSize)]\n",
    "\n",
    "    # 最新板情報を更新\n",
    "    cur = 0\n",
    "    for ex in EXC:\n",
    "      for ab in [self.asks, self.bids]:\n",
    "        for dep in range(3):\n",
    "          # 金額\n",
    "          board[cur] = ab[ex][cnt:cnt+1][0][dep][0]\n",
    "          cur += 1\n",
    "          # 量\n",
    "          board[cur] = ab[ex][cnt:cnt+1][0][dep][1]\n",
    "          cur += 1\n",
    "\n",
    "    print(action)\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action != 0:\n",
    "      # 売買 次のフレームの価格で購入できる\n",
    "      exidx = (action - 1) % EXLEN\n",
    "      ex = EXC[exidx]\n",
    "      buy = action <= EXLEN\n",
    "      if buy:\n",
    "        target = self.asks[ex][cnt+1:cnt+2][0]\n",
    "        # 買える分だけ買う\n",
    "        remain = BUYAMOUNT\n",
    "        for dep in range(3):\n",
    "          price = target[dep][0]\n",
    "          amount = target[dep][1]\n",
    "          if remain <= amount:\n",
    "            amount = remain\n",
    "          jpy = price * amount\n",
    "          if jpyBal[exidx] < jpy:\n",
    "            break\n",
    "          jpyBal[exidx] -= jpy\n",
    "          btcBal[exidx] += amount\n",
    "          remain -= amount\n",
    "          if remain <= 0:\n",
    "            break\n",
    "      else:\n",
    "        target = self.bids[ex][cnt+1:cnt+2][0]\n",
    "        # 売れる分だけ売る\n",
    "        remain = BUYAMOUNT\n",
    "        for dep in range(3):\n",
    "          price = target[dep][0]\n",
    "          amount = target[dep][1]\n",
    "          if remain <= amount:\n",
    "            amount = remain\n",
    "          if btcBal[exidx] < amount:\n",
    "            break\n",
    "          jpyBal[exidx] += price * amount\n",
    "          btcBal[exidx] -= amount\n",
    "          remain -= amount\n",
    "          if remain <= 0:\n",
    "            break\n",
    "\n",
    "    # state更新\n",
    "    self.state = jpyBal + btcBal + board\n",
    "\n",
    "    # 3秒時間を進める\n",
    "    self.stepCnt += 1\n",
    "    if self.stepCnt >= self.maxCnt:\n",
    "      self.episode_ended = True\n",
    "\n",
    "    # 最大128フレームで終了\n",
    "    frameCnt = self.stepCnt - self.startCnt\n",
    "    if frameCnt > 128:\n",
    "      self.episode_ended = True\n",
    "    \n",
    "    reward = 1\n",
    "    if frameCnt > 1:\n",
    "      jpy = self.valuationJPY(self.state)\n",
    "      btc = self.valuationBTC(self.state)\n",
    "      # 初期所持金から何倍増えたかが報酬\n",
    "      reward = jpy / self.initJPY + btc / self.initBTC - 2\n",
    "\n",
    "    # 5%所持金が減ったら強制終了\n",
    "    #if reward < -0.1:\n",
    "    #  self._episode_ended = True\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array(self.state).astype(np.float32), reward*128, self.episode_ended, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\"render\")\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100):\n",
    "    \"\"\"\n",
    "    RLエージェントを評価\n",
    "    :param model: (BaseRLModel object) RLエージェント\n",
    "    :param num_episodes: (int) エピソード数\n",
    "    :return: (float) 平均報酬\n",
    "    \"\"\"\n",
    "    # この関数は単一の環境でのみ機能します\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _statesは、LSTMポリシーを使用する場合にのみ有用です\n",
    "            action, _states = model.predict(obs)\n",
    "\n",
    "            # ベクトル化環境を使用しているため、行動、報酬、エピソード完了は配列です\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n2\n7\n7\n1\n8\n8\n7\n4\n8\n1\n7\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002040766 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.1        |\n",
      "|    explained_variance   | -1.83e+08   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.05e+05    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 9.49e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017879775 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | -1.48e+06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.99e+04    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 5.08e+04    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1140705e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.18         |\n",
      "|    explained_variance   | -3.4e+04      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.07e+05      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00281      |\n",
      "|    value_loss           | 5.75e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 515          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013040134 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | -3.86e+06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.83e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 5.71e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066900663 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | -1.38e+07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.46e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 3.74e+05     |\n",
      "------------------------------------------\n",
      "Mean reward: 7442.1133 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "mean_reward = evaluate(model, num_episodes=100)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean reward: 6621.2793 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "mean_reward = evaluate(model, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}